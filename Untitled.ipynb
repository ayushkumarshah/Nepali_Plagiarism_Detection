{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#split at ?, । or !\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_sentence(text):\n",
    "    sentences=re.split('(?<=[।?!]) +', text)\n",
    "    return sentences\n",
    "\n",
    "def tokenize_word(sentences):\n",
    "    words=[]\n",
    "    for sentence in sentences:\n",
    "        words.extend(re.split(', |,| ', sentence))\n",
    "    return words\n",
    "\n",
    "def clean_text(words):\n",
    "    punctuations=r',|\\)|\\(|\\{|\\}|\\[|\\]|\\?|\\!|।|\\‘|\\’|\\“|\\”|\\:-|/|—|-'\n",
    "    numbers = r'[0-9o१२३४५६७८९]'\n",
    "    words = [re.sub(numbers, '', i) for i in words]\n",
    "    words = [re.sub(punctuations, '', i) for i in words]\n",
    "    #Removing empty strings\n",
    "    words = [x for x in words if x]\n",
    "    return words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    with open('datasets/stopwords.txt',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines]\n",
    "#     print(\"Stopwords dataset\")\n",
    "#     print(lines)\n",
    "    words = [w for w in words if not w in lines]\n",
    "    return words\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences=tokenize_sentence(text)\n",
    "    words=tokenize_word(sentences)\n",
    "    words=clean_text(words)\n",
    "    words=remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def calculate_TF_Dict(doc):\n",
    "    \"\"\" Returns a tf dictionary for each doc whose keys are all\n",
    "    the unique words in the review and whose values are their\n",
    "    corresponding tf.\n",
    "    \"\"\"\n",
    "    TF_Dict = {}\n",
    "\n",
    "    # Total number of terms in the document\n",
    "    len_of_document = float(len(doc))\n",
    "\n",
    "    for word in doc:\n",
    "\n",
    "        # Number of times the term occurs in the document\n",
    "        term_in_document = doc.count(word)\n",
    "\n",
    "        #Computes tf for each word\n",
    "        TF_Dict[word] = term_in_document / len_of_document\n",
    "\n",
    "\n",
    "    return TF_Dict\n",
    "\n",
    "def calculate_IDF_Dict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    for word in vocabulary:\n",
    "        countDict[word]=0\n",
    "    # Run through each doc's tf dictionary and increment countDict's (word, doc) pair\n",
    "    for word in vocabulary:\n",
    "        for doc in range(len(Docs)):\n",
    "            if word in Docs[doc]:\n",
    "                countDict[word]+=1\n",
    "\n",
    "\n",
    "    #Stores the doc count dictionary\n",
    "    IDF_Dict = {}\n",
    "    total_num_docs = len(Docs)\n",
    "    for word in countDict:\n",
    "        IDF_Dict[word] = 1+math.log(float(total_num_docs) /countDict[word])\n",
    "    return IDF_Dict\n",
    "\n",
    "def calculate_TFIDF_Dict(TF_Dict,IDF_Dict):\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    review and whose values are their corresponding tfidf.\n",
    "    \"\"\"\n",
    "    TFIDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for word in TF_Dict:\n",
    "        TFIDF_Dict[word] = TF_Dict[word] * IDF_Dict[word]\n",
    "    return TFIDF_Dict\n",
    "\n",
    "\n",
    "def calculate_TFIDF_Vector(doc):\n",
    "    TFIDF_Vector = [0.0] * len(vocabulary)\n",
    "    # For each unique word, if it is in the doc, store its TF-IDF value.\n",
    "    for i, word in enumerate(vocabulary):\n",
    "          if word in doc:\n",
    "                TFIDF_Vector[i] = doc[word]\n",
    "\n",
    "    return TFIDF_Vector\n",
    "\n",
    "def create_vector():\n",
    "    TF_Dict = [calculate_TF_Dict(doc) for doc in Docs]\n",
    "    IDF_Dict=calculate_IDF_Dict()\n",
    "    TFIDF_Dict = [calculate_TFIDF_Dict(doc,IDF_Dict) for doc in TF_Dict]\n",
    "    TFIDF_Vector = [calculate_TFIDF_Vector(doc) for doc in TFIDF_Dict]\n",
    "    return TFIDF_Vector\n",
    "\n",
    "\n",
    "def get_jaccard_sim(token1, token2):\n",
    "    a = set(token1)\n",
    "    b = set(token2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def get_cosine_sim(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    with open (path,  encoding=\"utf8\") as myfile:\n",
    "        return myfile.read()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(eg1,eg2):\n",
    "    tokens1=preprocess(eg1)\n",
    "    tokens2=preprocess(eg2)\n",
    "    global vocabulary\n",
    "    vocabulary=sorted(set(tokens1+tokens2))\n",
    "    global Docs\n",
    "    Docs=[tokens1,tokens2]\n",
    "    TFIDF_Vector=create_vector()\n",
    "    jsim=get_jaccard_sim(tokens1,tokens2)\n",
    "    csim=get_cosine_sim(TFIDF_Vector[0],TFIDF_Vector[1])\n",
    "    return(jsim,csim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_list(basepath):\n",
    "    import os\n",
    "    mylist = []\n",
    "    path=[]\n",
    "    data=[]\n",
    "\n",
    "    \n",
    "    for entry in os.listdir(basepath):\n",
    "        if os.path.isfile(os.path.join(basepath, entry)):\n",
    "            mylist.append(entry)\n",
    "\n",
    "\n",
    "    for i in range(0,len(mylist)):\n",
    "        path.append('D:/nlp_project/Nepali_Plagiarism_Detection/datasets/data/'+mylist[i])\n",
    "\n",
    "    from itertools import combinations \n",
    "    comb = list(combinations(path, 2)) \n",
    "    dic={}\n",
    "    dic_cosim={}\n",
    "    for com in comb:\n",
    "        between=\"between\"+com[0].replace( 'D:/nlp_project/Nepali_Plagiarism_Detection/datasets/data/','')+ ' and '+com[1].replace('D:/nlp_project/Nepali_Plagiarism_Detection/datasets/data/','')\n",
    "        sim=similarity(read(com[0]),read(com[1]))\n",
    "        dic[between]=sim\n",
    "        dic_cosim[between]=sim[1]\n",
    "\n",
    "    sorted_x = sorted(dic_cosim.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    final_list=[]\n",
    "    for i in sorted_x:\n",
    "        final_list.append(i[0]+\" \"+str(dic[i[0]]))\n",
    "\n",
    "    return(final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['betweenstudent1.txt and student2.txt (0.08453608247422681, 0.170638838882139)',\n",
       " 'betweenstudent2.txt and student3.txt (0.033854166666666664, 0.05768637529541426)',\n",
       " 'betweenstudent1.txt and student3.txt (0.037815126050420166, 0.04171463866935128)']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_final_list('D:/nlp_project/Nepali_Plagiarism_Detection/datasets/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
