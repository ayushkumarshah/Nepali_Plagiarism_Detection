{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences\n",
      "['परिश्रम नगरी हुन्छ?', 'परिश्रम सफलताको ९९ एक-मात्र बाटो हो।', 'जो ९ ९९९ ९९ परिश्रम गर्छ, उही सफल हुन्छ।', 'अब त परिश्रम गर्छौ नि?', 'नगरी कहाँ हुन्छ त!']\n",
      "Words after tokenization\n",
      "['परिश्रम', 'नगरी', 'हुन्छ?', 'परिश्रम', 'सफलताको', '९९', 'एक-मात्र', 'बाटो', 'हो।', 'जो', '९', '९९९', '९९', 'परिश्रम', 'गर्छ', 'उही', 'सफल', 'हुन्छ।', 'अब', 'त', 'परिश्रम', 'गर्छौ', 'नि?', 'नगरी', 'कहाँ', 'हुन्छ', 'त!']\n",
      "After removing special symbols and numbers\n",
      "['परिश्रम', 'नगरी', 'हुन्छ', 'परिश्रम', 'सफलताको', 'एकमात्र', 'बाटो', 'हो', 'जो', 'परिश्रम', 'गर्छ', 'उही', 'सफल', 'हुन्छ', 'अब', 'त', 'परिश्रम', 'गर्छौ', 'नि', 'नगरी', 'कहाँ', 'हुन्छ', 'त']\n",
      "Words after removing stopwords\n",
      "['परिश्रम', 'नगरी', 'परिश्रम', 'सफलताको', 'एकमात्र', 'परिश्रम', 'उही', 'सफल', 'परिश्रम', 'गर्छौ', 'नगरी']\n",
      "Sentences\n",
      "['परिश्रम नगरी हुन्छ?', 'परिश्रम सफलताको बाटो हो।', 'जो परिश्रम गर्छ, उही सफल हुन्छ।', 'अब परिश्रम गर्छौ नि?', 'नगरी हुन्छ त!']\n",
      "Words after tokenization\n",
      "['परिश्रम', 'नगरी', 'हुन्छ?', 'परिश्रम', 'सफलताको', 'बाटो', 'हो।', 'जो', 'परिश्रम', 'गर्छ', 'उही', 'सफल', 'हुन्छ।', 'अब', 'परिश्रम', 'गर्छौ', 'नि?', 'नगरी', 'हुन्छ', 'त!']\n",
      "After removing special symbols and numbers\n",
      "['परिश्रम', 'नगरी', 'हुन्छ', 'परिश्रम', 'सफलताको', 'बाटो', 'हो', 'जो', 'परिश्रम', 'गर्छ', 'उही', 'सफल', 'हुन्छ', 'अब', 'परिश्रम', 'गर्छौ', 'नि', 'नगरी', 'हुन्छ', 'त']\n",
      "Words after removing stopwords\n",
      "['परिश्रम', 'नगरी', 'परिश्रम', 'सफलताको', 'परिश्रम', 'उही', 'सफल', 'परिश्रम', 'गर्छौ', 'नगरी']\n",
      "['परिश्रम', 'नगरी', 'परिश्रम', 'सफलताको', 'एकमात्र', 'परिश्रम', 'उही', 'सफल', 'परिश्रम', 'गर्छौ', 'नगरी']\n",
      "['परिश्रम', 'नगरी', 'परिश्रम', 'सफलताको', 'परिश्रम', 'उही', 'सफल', 'परिश्रम', 'गर्छौ', 'नगरी']\n",
      "[[0.09090909090909091, 0.15392247095999503, 0.09090909090909091, 0.18181818181818182, 0.36363636363636365, 0.09090909090909091, 0.09090909090909091], [0.1, 0.0, 0.1, 0.2, 0.4, 0.1, 0.1]]\n",
      "0.8571428571428571\n",
      "0.9451442030944225\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#split at ?, । or !\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_sentence(text):\n",
    "    sentences=re.split('(?<=[।?!]) +', text)\n",
    "    print(\"Sentences\")\n",
    "    print(sentences)\n",
    "    return sentences\n",
    "\n",
    "def tokenize_word(sentences):\n",
    "    words=[]\n",
    "    for sentence in sentences:\n",
    "        words.extend(re.split(', |,| ', sentence))\n",
    "    print(\"Words after tokenization\")\n",
    "    print (words)\n",
    "    return words\n",
    "\n",
    "def clean_text(words):\n",
    "    punctuations=r',|\\)|\\(|\\{|\\}|\\[|\\]|\\?|\\!|।|\\‘|\\’|\\“|\\”|\\:-|/|—|-'\n",
    "    numbers = r'[0-9o१२३४५६७८९]'\n",
    "    words = [re.sub(numbers, '', i) for i in words]\n",
    "    words = [re.sub(punctuations, '', i) for i in words]\n",
    "    #Removing empty strings\n",
    "    words = [x for x in words if x]\n",
    "    print(\"After removing special symbols and numbers\")\n",
    "    print(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    with open('datasets/stopwords.txt',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines]\n",
    "#     print(\"Stopwords dataset\")\n",
    "#     print(lines)\n",
    "    words = [w for w in words if not w in lines]\n",
    "    print(\"Words after removing stopwords\")\n",
    "    print(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences=tokenize_sentence(text)\n",
    "    words=tokenize_word(sentences)\n",
    "    words=clean_text(words)\n",
    "    words=remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def calculate_TF_Dict(doc):\n",
    "    \"\"\" Returns a tf dictionary for each doc whose keys are all\n",
    "    the unique words in the review and whose values are their\n",
    "    corresponding tf.\n",
    "    \"\"\"\n",
    "    TF_Dict = {}\n",
    "\n",
    "    # Total number of terms in the document\n",
    "    len_of_document = float(len(doc))\n",
    "\n",
    "    for word in doc:\n",
    "\n",
    "        # Number of times the term occurs in the document\n",
    "        term_in_document = doc.count(word)\n",
    "\n",
    "        #Computes tf for each word\n",
    "        TF_Dict[word] = term_in_document / len_of_document\n",
    "\n",
    "\n",
    "    return TF_Dict\n",
    "\n",
    "def calculate_IDF_Dict():\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    dataset and whose values are their corresponding idf.\n",
    "    \"\"\"\n",
    "    countDict = {}\n",
    "    for word in vocabulary:\n",
    "        countDict[word]=0\n",
    "    # Run through each doc's tf dictionary and increment countDict's (word, doc) pair\n",
    "    for word in vocabulary:\n",
    "        for doc in range(len(Docs)):\n",
    "            if word in Docs[doc]:\n",
    "                countDict[word]+=1\n",
    "\n",
    "\n",
    "    #Stores the doc count dictionary\n",
    "    IDF_Dict = {}\n",
    "    total_num_docs = len(Docs)\n",
    "    for word in countDict:\n",
    "        IDF_Dict[word] = 1+math.log(float(total_num_docs) /countDict[word])\n",
    "    return IDF_Dict\n",
    "\n",
    "def calculate_TFIDF_Dict(TF_Dict,IDF_Dict):\n",
    "    \"\"\" Returns a dictionary whose keys are all the unique words in the\n",
    "    review and whose values are their corresponding tfidf.\n",
    "    \"\"\"\n",
    "    TFIDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for word in TF_Dict:\n",
    "        TFIDF_Dict[word] = TF_Dict[word] * IDF_Dict[word]\n",
    "    return TFIDF_Dict\n",
    "\n",
    "\n",
    "def calculate_TFIDF_Vector(doc):\n",
    "    TFIDF_Vector = [0.0] * len(vocabulary)\n",
    "    # For each unique word, if it is in the doc, store its TF-IDF value.\n",
    "    for i, word in enumerate(vocabulary):\n",
    "          if word in doc:\n",
    "                TFIDF_Vector[i] = doc[word]\n",
    "\n",
    "    return TFIDF_Vector\n",
    "\n",
    "def create_vector():\n",
    "    TF_Dict = [calculate_TF_Dict(doc) for doc in Docs]\n",
    "    IDF_Dict=calculate_IDF_Dict()\n",
    "    TFIDF_Dict = [calculate_TFIDF_Dict(doc,IDF_Dict) for doc in TF_Dict]\n",
    "    TFIDF_Vector = [calculate_TFIDF_Vector(doc) for doc in TFIDF_Dict]\n",
    "    return TFIDF_Vector\n",
    "\n",
    "\n",
    "def get_jaccard_sim(token1, token2):\n",
    "    a = set(token1)\n",
    "    b = set(token2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def get_cosine_sim(a,b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos\n",
    "\n",
    "eg1=\"परिश्रम नगरी हुन्छ? परिश्रम सफलताको ९९ एक-मात्र बाटो हो। जो ९ ९९९ ९९ परिश्रम गर्छ, उही सफल हुन्छ। अब त परिश्रम गर्छौ नि? नगरी कहाँ हुन्छ त!\"\n",
    "tokens1=preprocess(eg1)\n",
    "eg2=\"परिश्रम नगरी हुन्छ? परिश्रम सफलताको बाटो हो। जो परिश्रम गर्छ, उही सफल हुन्छ। अब परिश्रम गर्छौ नि? नगरी हुन्छ त!\"\n",
    "tokens2=preprocess(eg2)\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "vocabulary=sorted(set(tokens1+tokens2))\n",
    "Docs=[tokens1,tokens2]\n",
    "TFIDF_Vector=create_vector()\n",
    "print(TFIDF_Vector)\n",
    "jsim=get_jaccard_sim(tokens1,tokens2)\n",
    "csim=get_cosine_sim(TFIDF_Vector[0],TFIDF_Vector[1])\n",
    "print(jsim)\n",
    "print(csim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
